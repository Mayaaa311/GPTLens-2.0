[llama3]
temperature = 0
n_gpu_layers = -1
n_ctx = 4096

[llama2]
temperature = 0
n_gpu_layers = -1
n_ctx = 4096
model_path = "models/llama-2-7b.Q5_K_M.gguf"

[codellama]
temperature = 0
n_gpu_layers = -1
n_ctx = 4096

[codeqwen]
temperature = 0
n_gpu_layers = -1
n_ctx = 4096

[deepseek-coder-v2]
temperature = 0
n_gpu_layers = -1
n_ctx = 8000
